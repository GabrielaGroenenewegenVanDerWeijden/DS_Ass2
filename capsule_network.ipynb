{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Network model on Pneumonia dataset\n",
    "\n",
    "An adaptation from the MNIST baseline model created in the other file named such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "cutoff = 128  # 0 == use all\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsule_iterations = 3\n",
    "\n",
    "num_filters = 256\n",
    "num_base_mappings = 32\n",
    "dim_base_capsules = num_filters // num_base_mappings\n",
    "\n",
    "num_labels = 2\n",
    "dim_super_capsules = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "# img_size = 150\n",
    "img_size = 28\n",
    "\n",
    "def get_training_data(data_dir, img_size=150):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img_arr is None:\n",
    "                    continue\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    return np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set150 = get_training_data('Data/chest_xray/chest_xray/train', 150)\n",
    "test_set150 = get_training_data('Data/chest_xray/chest_xray/test', 150)\n",
    "\n",
    "train_set56 = get_training_data('Data/chest_xray/chest_xray/train', 56)\n",
    "test_set56 = get_training_data('Data/chest_xray/chest_xray/test', 56)\n",
    "\n",
    "train_set28 = get_training_data('Data/chest_xray/chest_xray/train', 28)\n",
    "test_set28 = get_training_data('Data/chest_xray/chest_xray/test', 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_size == 28:\n",
    "    train_set = train_set28\n",
    "    test_set = test_set28\n",
    "elif img_size == 56:\n",
    "    train_set = train_set56\n",
    "    test_set = test_set56\n",
    "else:\n",
    "    train_set = train_set150\n",
    "    test_set = test_set150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature, label \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[1;32m      4\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[1;32m      5\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for feature, label in train_set:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for feature, label in test_set:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test , y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5216, 28, 28, 1]), TensorShape([624, 28, 28, 1]))"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train) / 255.0\n",
    "X_train = tf.cast(X_train, dtype=tf.float32)\n",
    "X_train = tf.expand_dims(X_train, axis=-1)\n",
    "\n",
    "X_test = np.array(X_test) / 255.0\n",
    "X_test = tf.cast(X_test, dtype=tf.float32)\n",
    "X_test = tf.expand_dims(X_test, axis=-1)\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutoff data used for quicker development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller set for development and testing\n",
    "if cutoff > 0:\n",
    "    try:\n",
    "        X_train, y_train = X_train[:cutoff], y_train[:cutoff]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    try:\n",
    "        X_test, y_test = X_test[:cutoff], y_test[:cutoff]\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trainset = X_train.shape[0]\n",
    "n_testset = X_test.shape[0]\n",
    "\n",
    "dim_imgs = X_train.shape[1]\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n",
    "# dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# testset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# testset = testset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGenerator = tf.keras.preprocessing.image.ImageDataGenerator\n",
    "datagen = DataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False  # randomly flip images\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = datagen.flow(X_train, y_train)\n",
    "\n",
    "X_train = augmented_dataset.x\n",
    "y_train = augmented_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "testset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "testset = testset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output_imgsize(input_size: int, kernel_size: int = 9, strides: int = 1):\n",
    "    return (input_size - kernel_size) // strides\n",
    "\n",
    "def get_img_dims(img) -> int:\n",
    "    return img.shape[1], img.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalise(vector, axis=1, epsilon=1e-7, keepdims=True):\n",
    "    norm_squared = tf.reduce_sum(tf.square(vector), axis, keepdims)\n",
    "    return tf.sqrt(norm_squared + epsilon)  # epsilon added to prevent division by 0\n",
    "\n",
    "def safe_squash(vector, axis=1, epsilon=1e-7):\n",
    "    norm_squared = tf.reduce_sum(\n",
    "        tf.square(vector), \n",
    "        axis=axis, \n",
    "        keepdims=True\n",
    "    )\n",
    "    scalar_factor = norm_squared / (1 + norm_squared)\n",
    "    \n",
    "    safe_normalise = tf.sqrt(norm_squared + epsilon)  # epsilon added to prevent division by 0\n",
    "    unit_vector = vector / safe_normalise\n",
    "    return scalar_factor * unit_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(\n",
    "    vector, \n",
    "    reconstructed_image, \n",
    "    y, \n",
    "    y_image,\n",
    "    epsilon=1e-7,\n",
    "    m_plus=0.9,\n",
    "    m_minus=0.1,\n",
    "    lambda_=0.5,\n",
    "    alpha=0.0005,\n",
    "):\n",
    "    img_dim_w, img_dim_h = img_size, img_size\n",
    "    img_resolution = img_dim_w * img_dim_h\n",
    "\n",
    "    safe_normal = safe_normalise(vector, axis=-1, keepdims=True, epsilon=epsilon)\n",
    "    prediction = tf.reshape(safe_normal, [-1, num_labels])\n",
    "\n",
    "    left_margin = tf.square(tf.maximum(0.0, m_plus - prediction))\n",
    "    right_margin = tf.square(tf.maximum(0.0, prediction - m_minus))\n",
    "\n",
    "    margin_loss = tf.add(y * left_margin, lambda_ * (1.0 - y) * right_margin)\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(margin_loss, axis=-1))\n",
    "\n",
    "    y_image_flat = tf.reshape(y_image, [-1, img_resolution])\n",
    "    # print(f\"{y_image_flat.shape= }\")\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(y_image_flat - reconstructed_image))\n",
    "\n",
    "    loss = tf.add(margin_loss, alpha * reconstruction_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Capsule Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNetwork(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_filters, \n",
    "        num_base_mappings, dim_base_capsules, \n",
    "        num_labels, dim_super_capsules, \n",
    "        iterations: int = 3,\n",
    "        kernel_size: int = 9):\n",
    "        super().__init__()\n",
    "        self.num_filters = num_filters  # 256\n",
    "        self.num_base_mappings = num_base_mappings  # 32\n",
    "        self.dim_base_capsules = dim_base_capsules  # 8\n",
    "        self.num_labels = num_labels  # 10\n",
    "        self.dim_super_capsules = dim_super_capsules  # 16\n",
    "        self.num_base_capsules = self.num_base_mappings * 6 ** 2  # 1152\n",
    "\n",
    "        self.iterations = iterations\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        with tf.name_scope(\"Variables\") as scope:\n",
    "            kernel = [self.kernel_size, self.kernel_size]\n",
    "            \n",
    "            self.convolution = tf.keras.layers.Conv2D(self.num_filters, kernel, strides=[1,1], name='ConvolutionLayer', activation='relu')\n",
    "            self.base_capsule = tf.keras.layers.Conv2D(self.num_base_mappings * self.dim_base_capsules, kernel, strides=[2,2], name=\"BaseCapsule\")\n",
    "            self.w = tf.Variable(\n",
    "                tf.random_normal_initializer()(shape=[\n",
    "                    1, \n",
    "                    self.num_base_capsules, self.num_labels, \n",
    "                    self.dim_super_capsules, self.dim_base_capsules\n",
    "                    ]), \n",
    "                dtype=tf.float32, \n",
    "                name=\"PoseEstimation\", \n",
    "                trainable=True)\n",
    "            \n",
    "            self.dense_1 = tf.keras.layers.Dense(units = 512, activation='relu')\n",
    "            self.dense_2 = tf.keras.layers.Dense(units = 1024, activation='relu')\n",
    "            self.dense_3 = tf.keras.layers.Dense(units = 784, activation='sigmoid', dtype='float32')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def squash(self, vector, epsilon=1e-7):\n",
    "        with tf.name_scope(\"SafeSquashFunction\") as scope:\n",
    "            norm_squared = tf.reduce_sum(\n",
    "                tf.square(vector), \n",
    "                axis=-1, \n",
    "                keepdims=True\n",
    "            )\n",
    "            scalar_factor = norm_squared / (1 + norm_squared)\n",
    "            \n",
    "            safety_normalise = tf.sqrt(norm_squared + epsilon)\n",
    "            unit_vector = vector / safety_normalise\n",
    "            return scalar_factor * unit_vector\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        input_x, y = inputs\n",
    "        # input_x.shape: (None, 28, 28, 1)\n",
    "        # y.shape: (None, 10)\n",
    "\n",
    "        x = self.convolution(input_x) # x.shape: (None, 20, 20, 256)\n",
    "        x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "        # print(f\"{x.shape= }\")\n",
    "\n",
    "        with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "            u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "            b = tf.zeros((input_x.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "            for i in range(self.iterations): # self.iterations = 3\n",
    "                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "                # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "                b += agreement\n",
    "\n",
    "        with tf.name_scope(\"Masking\") as scope:\n",
    "            y = tf.expand_dims(y, axis=-1) # y.shape: (None, 10, 1)\n",
    "            y = tf.expand_dims(y, axis=1) # y.shape: (None, 1, 10, 1)\n",
    "            mask = tf.cast(y, dtype=tf.float32) # mask.shape: (None, 1, 10, 1)\n",
    "            v_masked = tf.multiply(mask, v) # v_masked.shape: (None, 1, 10, 16)\n",
    "\n",
    "        with tf.name_scope(\"Reconstruction\") as scope:\n",
    "            v_ = tf.reshape(v_masked, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "\n",
    "        return v, reconstructed_image\n",
    "\n",
    "    @tf.function\n",
    "    def predict_capsule_output(self, inputs):\n",
    "        x = self.convolution(inputs) # x.shape: (None, 20, 20, 256)\n",
    "        x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "\n",
    "        with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "            u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "            b = tf.zeros((inputs.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "            for i in range(self.iterations): # self.iterations = 3\n",
    "                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "                # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "                b += agreement\n",
    "        return v\n",
    "\n",
    "    @tf.function\n",
    "    def regenerate_image(self, inputs):\n",
    "        with tf.name_scope(\"Reconstruction\") as scope:\n",
    "            v_ = tf.reshape(inputs, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "        return reconstructed_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No `profiler_outdir` passed to trace_on(). Profiler won't be enabled.\n"
     ]
    }
   ],
   "source": [
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "model = CapsuleNetwork(\n",
    "    num_filters=num_filters,\n",
    "    num_base_mappings=num_base_mappings,\n",
    "    dim_base_capsules=dim_base_capsules,\n",
    "    num_labels=num_labels,\n",
    "    dim_super_capsules=dim_super_capsules,\n",
    "    iterations=capsule_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimiser = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n"
     ]
    }
   ],
   "source": [
    "def train(X, y) -> float:\n",
    "    y_one_hot = tf.one_hot(y, depth=num_labels)\n",
    "    with tf.GradientTape() as tape:\n",
    "        vector, reconstructed_image = model([X, y_one_hot])\n",
    "        loss = loss_function(vector, reconstructed_image, y_one_hot, X)\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    Optimiser.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# _ = train(X_train[:32], y_train[:32])\n",
    "\n",
    "tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    pred = model.predict_capsule_output(x)\n",
    "    pred_normed = safe_normalise(pred)\n",
    "    pred_normed = tf.squeeze(pred_normed, [1])\n",
    "    return np.argmax(pred_normed, axis=1)[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s, Loss : 0.49226343631744385 Accuracy : 1.0]         \n",
      "Epoch 2/2: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s, Loss : 0.4042533040046692 Accuracy : 1.0]         \n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "losses = []\n",
    "accuracy = []\n",
    "for i in range(1, epochs+1, 1):\n",
    "\n",
    "    loss = 0\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "\n",
    "        description = f\"Epoch {i}/{epochs}\"\n",
    "        pbar.set_description_str(description)\n",
    "\n",
    "        for X_batch, y_batch in dataset:\n",
    "\n",
    "            loss += train(X_batch, y_batch)\n",
    "            pbar.update(1)\n",
    "\n",
    "        loss /= len(dataset)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        training_sum = 0\n",
    "\n",
    "        print_statement = f\"Loss : {loss.numpy()}\" + \" Evaluating Accuracy ...\"\n",
    "        pbar.set_postfix_str(print_statement)\n",
    "\n",
    "        for X_batch, y_batch in dataset:\n",
    "            training_sum += sum(predict(model, X_batch)==y_batch.numpy())\n",
    "        accuracy.append(training_sum/n_trainset)\n",
    "\n",
    "        print_statement = f\"Loss : {loss.numpy()} Accuracy : {accuracy[-1]}\"\n",
    "\n",
    "        pbar.set_postfix_str(print_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_sum = 0\n",
    "for X_batch, y_batch in testset:\n",
    "    test_sum += sum(predict(model, X_batch)==y_batch.numpy())\n",
    "print(test_sum/n_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/capsnet_tiny.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"capsule_network_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"capsule_network_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ConvolutionLayer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BaseCapsule (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">803,600</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ConvolutionLayer (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m20,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BaseCapsule (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m5,308,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m784\u001b[0m)              │       \u001b[38;5;34m803,600\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,675,472</span> (25.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,675,472\u001b[0m (25.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,675,472</span> (25.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,675,472\u001b[0m (25.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 28, 28, 1)\n",
      "(5, 1, 2, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "test_sample = X_test[:n_samples]\n",
    "features = model.predict_capsule_output(test_sample)\n",
    "\n",
    "temp_features = features.numpy()\n",
    "temp_ = temp_features.copy()\n",
    "\n",
    "\n",
    "label_selection = num_labels\n",
    "plt.figure(figsize=(label_selection * 2, 3))\n",
    "for idx in range(label_selection):\n",
    "    pred = predict(model, tf.expand_dims(X_test[idx], axis=0))\n",
    "\n",
    "    temp_features[:,:,:,:] = 0\n",
    "    temp_features[idx:,:,idx,:] = temp_[idx:,:,idx,:]\n",
    "\n",
    "    reconstruction = model.regenerate_image(temp_features[idx])\n",
    "    reconstruction = tf.reshape(reconstruction, (img_size, img_size))\n",
    "\n",
    "    plt.subplot(1, label_selection, idx + 1)\n",
    "    plt.imshow(reconstruction, cmap=\"gray\")\n",
    "    plt.title(f\"Predicted: {pred}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "plt.figure(figsize=(label_selection * 2, 3))\n",
    "for idx in range(label_selection):\n",
    "    label = y_test[idx]\n",
    "    plt.subplot(1, label_selection, idx + 1)\n",
    "    plt.imshow(test_sample[idx], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 4\n",
    "\n",
    "# col = np.zeros((img_size, 308))\n",
    "# for i in range(16): \n",
    "#     feature_ = temp_features.copy()\n",
    "#     feature_[:,:,idx, i] += -0.25\n",
    "#     row = np.zeros((img_size, img_size))\n",
    "#     for j in range(10):\n",
    "#         feature_[:,:,idx, i] += 0.05\n",
    "#         feature_ = tf.convert_to_tensor(feature_)\n",
    "#         print(feature_.shape)\n",
    "#         row = np.hstack([\n",
    "#             row, \n",
    "#             tf.reshape(model.regenerate_image(feature_), \n",
    "#             (img_size, img_size)).numpy()\n",
    "#         ])\n",
    "#     col = np.vstack([col, row])\n",
    "    \n",
    "# plt.figure(figsize=(30,20))\n",
    "# plt.imshow(col[img_size:, img_size:], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Network model on Pneumonia dataset\n",
    "\n",
    "An adaptation from the MNIST baseline model created in the other file named such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "cutoff = 0  # 0 == use all\n",
    "\n",
    "epochs = 12\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsule_iterations = 3\n",
    "\n",
    "num_filters = 256\n",
    "num_base_mappings = 32\n",
    "dim_base_capsules = num_filters // num_base_mappings\n",
    "\n",
    "num_labels = 2\n",
    "dim_super_capsules = 512 // num_base_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "# img_size = 150\n",
    "img_size = 28\n",
    "\n",
    "def get_training_data(data_dir, img_size=150):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img_arr is None:\n",
    "                    continue\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    return np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set150 = get_training_data('Data/chest_xray/chest_xray/train', 150)\n",
    "# test_set150 = get_training_data('Data/chest_xray/chest_xray/test', 150)\n",
    "\n",
    "# train_set56 = get_training_data('Data/chest_xray/chest_xray/train', 56)\n",
    "# test_set56 = get_training_data('Data/chest_xray/chest_xray/test', 56)\n",
    "\n",
    "# train_set28 = get_training_data('Data/chest_xray/chest_xray/train', 28)\n",
    "# test_set28 = get_training_data('Data/chest_xray/chest_xray/test', 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_size == 28:\n",
    "    train_set = train_set28\n",
    "    test_set = test_set28\n",
    "elif img_size == 56:\n",
    "    train_set = train_set56\n",
    "    test_set = test_set56\n",
    "else:\n",
    "    train_set = train_set150\n",
    "    test_set = test_set150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for feature, label in train_set:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for feature, label in test_set:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test , y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5216, 28, 28, 1]), TensorShape([624, 28, 28, 1]))"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train) / 255.0\n",
    "X_train = tf.cast(X_train, dtype=tf.float32)\n",
    "X_train = tf.expand_dims(X_train, axis=-1)\n",
    "\n",
    "X_test = np.array(X_test) / 255.0\n",
    "X_test = tf.cast(X_test, dtype=tf.float32)\n",
    "X_test = tf.expand_dims(X_test, axis=-1)\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutoff data used for quicker development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller set for development and testing\n",
    "if cutoff > 0:\n",
    "    try:\n",
    "        X_train, y_train = X_train[:cutoff], y_train[:cutoff]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    try:\n",
    "        X_test, y_test = X_test[:cutoff], y_test[:cutoff]\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trainset = X_train.shape[0]\n",
    "n_testset = X_test.shape[0]\n",
    "\n",
    "dim_imgs = X_train.shape[1]\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n",
    "# dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# testset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# testset = testset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGenerator = tf.keras.preprocessing.image.ImageDataGenerator\n",
    "datagen = DataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False  # randomly flip images\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = datagen.flow(X_train, y_train)\n",
    "\n",
    "X_train = augmented_dataset.x\n",
    "y_train = augmented_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "testset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "testset = testset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output_imgsize(input_size: int, kernel_size: int = 9, strides: int = 1):\n",
    "    return (input_size - kernel_size) // strides\n",
    "\n",
    "def get_img_dims(img) -> int:\n",
    "    return img.shape[1], img.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalise(vector, axis=1, epsilon=1e-7, keepdims=True):\n",
    "    norm_squared = tf.reduce_sum(tf.square(vector), axis, keepdims)\n",
    "    return tf.sqrt(norm_squared + epsilon)  # epsilon added to prevent division by 0\n",
    "\n",
    "def safe_squash(vector, axis=1, epsilon=1e-7):\n",
    "    norm_squared = tf.reduce_sum(\n",
    "        tf.square(vector), \n",
    "        axis=axis, \n",
    "        keepdims=True\n",
    "    )\n",
    "    scalar_factor = norm_squared / (1 + norm_squared)\n",
    "    \n",
    "    safe_normalise = tf.sqrt(norm_squared + epsilon)  # epsilon added to prevent division by 0\n",
    "    unit_vector = vector / safe_normalise\n",
    "    return scalar_factor * unit_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(\n",
    "    vector, \n",
    "    reconstructed_image, \n",
    "    y, \n",
    "    y_image,\n",
    "    epsilon=1e-7,\n",
    "    m_plus=0.9,\n",
    "    m_minus=0.1,\n",
    "    lambda_=0.5,\n",
    "    alpha=0.0005,\n",
    "):\n",
    "    img_dim_w, img_dim_h = img_size, img_size\n",
    "    img_resolution = img_dim_w * img_dim_h\n",
    "\n",
    "    safe_normal = safe_normalise(vector, axis=-1, keepdims=True, epsilon=epsilon)\n",
    "    prediction = tf.reshape(safe_normal, [-1, num_labels])\n",
    "\n",
    "    left_margin = tf.square(tf.maximum(0.0, m_plus - prediction))\n",
    "    right_margin = tf.square(tf.maximum(0.0, prediction - m_minus))\n",
    "\n",
    "    margin_loss = tf.add(y * left_margin, lambda_ * (1.0 - y) * right_margin)\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(margin_loss, axis=-1))\n",
    "\n",
    "    y_image_flat = tf.reshape(y_image, [-1, img_resolution])\n",
    "    # print(f\"{y_image_flat.shape= }\")\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(y_image_flat - reconstructed_image))\n",
    "\n",
    "    loss = tf.add(margin_loss, alpha * reconstruction_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Capsule Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CapsuleNetwork(tf.keras.Model):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         num_filters, \n",
    "#         num_base_mappings, dim_base_capsules, \n",
    "#         num_labels, dim_super_capsules, \n",
    "#         iterations: int = 3,\n",
    "#         kernel_size: int = 9):\n",
    "#         super().__init__()\n",
    "#         self.num_filters = num_filters  # 256\n",
    "#         self.num_base_mappings = num_base_mappings  # 32\n",
    "#         self.dim_base_capsules = dim_base_capsules  # 8\n",
    "#         self.num_labels = num_labels  # 10\n",
    "#         self.dim_super_capsules = dim_super_capsules  # 16\n",
    "#         self.num_base_capsules = self.num_base_mappings * 6 ** 2  # 1152\n",
    "\n",
    "#         self.iterations = iterations\n",
    "#         self.kernel_size = kernel_size\n",
    "\n",
    "#         with tf.name_scope(\"Variables\") as scope:\n",
    "#             kernel = [self.kernel_size, self.kernel_size]\n",
    "            \n",
    "#             self.convolution = tf.keras.layers.Conv2D(self.num_filters, kernel, strides=[1,1], name='ConvolutionLayer', activation='relu')\n",
    "#             self.base_capsule = tf.keras.layers.Conv2D(self.num_base_mappings * self.dim_base_capsules, kernel, strides=[2,2], name=\"BaseCapsule\")\n",
    "#             self.w = tf.Variable(\n",
    "#                 tf.random_normal_initializer()(shape=[\n",
    "#                     1, \n",
    "#                     self.num_base_capsules, self.num_labels, \n",
    "#                     self.dim_super_capsules, self.dim_base_capsules\n",
    "#                     ]), \n",
    "#                 dtype=tf.float32, \n",
    "#                 name=\"PoseEstimation\", \n",
    "#                 trainable=True)\n",
    "            \n",
    "#             self.dense_1 = tf.keras.layers.Dense(units = 512, activation='relu')\n",
    "#             self.dense_2 = tf.keras.layers.Dense(units = 1024, activation='relu')\n",
    "#             self.dense_3 = tf.keras.layers.Dense(units = img_size**2, activation='sigmoid', dtype='float32')\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         pass\n",
    "\n",
    "#     def squash(self, vector, epsilon=1e-7):\n",
    "#         with tf.name_scope(\"SafeSquashFunction\") as scope:\n",
    "#             norm_squared = tf.reduce_sum(\n",
    "#                 tf.square(vector), \n",
    "#                 axis=-1, \n",
    "#                 keepdims=True\n",
    "#             )\n",
    "#             scalar_factor = norm_squared / (1 + norm_squared)\n",
    "            \n",
    "#             safety_normalise = tf.sqrt(norm_squared + epsilon)\n",
    "#             unit_vector = vector / safety_normalise\n",
    "#             return scalar_factor * unit_vector\n",
    "\n",
    "#     @tf.function\n",
    "#     def call(self, inputs):\n",
    "#         input_x, y = inputs\n",
    "#         # input_x.shape: (None, 28, 28, 1)\n",
    "#         # y.shape: (None, 10)\n",
    "\n",
    "#         x = self.convolution(input_x) # x.shape: (None, 20, 20, 256)\n",
    "#         x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "#         # print(f\"{x.shape= }\")\n",
    "\n",
    "#         with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "#             u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "#             u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "#             u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "#             u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "#             u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "#         with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "#             b = tf.zeros((input_x.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "#             for i in range(self.iterations): # self.iterations = 3\n",
    "#                 c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "#                 s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "#                 v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "#                 agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "#                 # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "#                 # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "#                 # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "#                 # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "#                 # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "#                 # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "#                 b += agreement\n",
    "\n",
    "#         with tf.name_scope(\"Masking\") as scope:\n",
    "#             y = tf.expand_dims(y, axis=-1) # y.shape: (None, 10, 1)\n",
    "#             y = tf.expand_dims(y, axis=1) # y.shape: (None, 1, 10, 1)\n",
    "#             mask = tf.cast(y, dtype=tf.float32) # mask.shape: (None, 1, 10, 1)\n",
    "#             v_masked = tf.multiply(mask, v) # v_masked.shape: (None, 1, 10, 16)\n",
    "\n",
    "#         with tf.name_scope(\"Reconstruction\") as scope:\n",
    "#             v_ = tf.reshape(v_masked, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "#             reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "#             reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "#             reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "\n",
    "#         return v, reconstructed_image\n",
    "\n",
    "#     @tf.function\n",
    "#     def predict_capsule_output(self, inputs):\n",
    "#         x = self.convolution(inputs) # x.shape: (None, 20, 20, 256)\n",
    "#         x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "\n",
    "#         with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "#             u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "#             u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "#             u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "#             u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "#             u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "#         with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "#             b = tf.zeros((inputs.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "#             for i in range(self.iterations): # self.iterations = 3\n",
    "#                 c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "#                 s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "#                 v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "#                 agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "#                 # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "#                 # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "#                 # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "#                 # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "#                 # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "#                 # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "#                 b += agreement\n",
    "#         return v\n",
    "\n",
    "#     @tf.function\n",
    "#     def regenerate_image(self, inputs):\n",
    "#         with tf.name_scope(\"Reconstruction\") as scope:\n",
    "#             v_ = tf.reshape(inputs, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "#             reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "#             reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "#             reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "#         return reconstructed_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNetwork(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_filters, \n",
    "        num_base_mappings, dim_base_capsules, \n",
    "        num_labels, dim_super_capsules, \n",
    "        iterations: int = 3,\n",
    "        kernel_size: int = 9):\n",
    "        super().__init__()\n",
    "        self.num_filters = num_filters  # 256\n",
    "        self.num_base_mappings = num_base_mappings  # 32\n",
    "        self.dim_base_capsules = dim_base_capsules  # 8\n",
    "        self.num_labels = num_labels  # 10\n",
    "        self.dim_super_capsules = dim_super_capsules  # 16\n",
    "        self.num_base_capsules = self.num_base_mappings * 6 ** 2  # 1152\n",
    "\n",
    "        self.iterations = iterations\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        with tf.name_scope(\"Variables\") as scope:\n",
    "            kernel = [self.kernel_size, self.kernel_size]\n",
    "            \n",
    "            self.convolution = tf.keras.layers.Conv2D(self.num_filters, kernel, strides=[1,1], name='ConvolutionLayer', activation='relu')\n",
    "            self.base_capsule = tf.keras.layers.Conv2D(self.num_base_mappings * self.dim_base_capsules, kernel, strides=[2,2], name=\"BaseCapsule\")\n",
    "            self.w = tf.Variable(\n",
    "                tf.random_normal_initializer()(shape=[\n",
    "                    1, \n",
    "                    self.num_base_capsules, self.num_labels, \n",
    "                    self.dim_super_capsules, self.dim_base_capsules\n",
    "                    ]), \n",
    "                dtype=tf.float32, \n",
    "                name=\"PoseEstimation\", \n",
    "                trainable=True)\n",
    "            \n",
    "            self.dense_1 = tf.keras.layers.Dense(units = 512, activation='relu')\n",
    "            self.dense_2 = tf.keras.layers.Dense(units = 1024, activation='relu')\n",
    "            self.dense_3 = tf.keras.layers.Dense(units = img_size**2, activation='sigmoid', dtype='float32')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def squash(self, vector, epsilon=1e-7):\n",
    "        with tf.name_scope(\"SafeSquashFunction\") as scope:\n",
    "            norm_squared = tf.reduce_sum(\n",
    "                tf.square(vector), \n",
    "                axis=-1, \n",
    "                keepdims=True\n",
    "            )\n",
    "            scalar_factor = norm_squared / (1 + norm_squared)\n",
    "            \n",
    "            safety_normalise = tf.sqrt(norm_squared + epsilon)\n",
    "            unit_vector = vector / safety_normalise\n",
    "            return scalar_factor * unit_vector\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        input_x, y = inputs\n",
    "        # input_x.shape: (None, 28, 28, 1)\n",
    "        # y.shape: (None, 10)\n",
    "\n",
    "        x = self.convolution(input_x) # x.shape: (None, 20, 20, 256)\n",
    "        x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "        # print(f\"{x.shape= }\")\n",
    "\n",
    "        with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "            u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "            b = tf.zeros((input_x.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "            for i in range(self.iterations): # self.iterations = 3\n",
    "                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "                # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "                b += agreement\n",
    "\n",
    "        with tf.name_scope(\"Masking\") as scope:\n",
    "            y = tf.expand_dims(y, axis=-1) # y.shape: (None, 10, 1)\n",
    "            y = tf.expand_dims(y, axis=1) # y.shape: (None, 1, 10, 1)\n",
    "            mask = tf.cast(y, dtype=tf.float32) # mask.shape: (None, 1, 10, 1)\n",
    "            v_masked = tf.multiply(mask, v) # v_masked.shape: (None, 1, 10, 16)\n",
    "\n",
    "        with tf.name_scope(\"Reconstruction\") as scope:\n",
    "            v_ = tf.reshape(v_masked, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "\n",
    "        return v, reconstructed_image\n",
    "\n",
    "    @tf.function\n",
    "    def predict_capsule_output(self, inputs):\n",
    "        x = self.convolution(inputs) # x.shape: (None, 20, 20, 256)\n",
    "        x = self.base_capsule(x) # x.shape: (None, 6, 6, 256)\n",
    "\n",
    "        with tf.name_scope(\"CapsuleFormation\") as scope:\n",
    "            u = tf.reshape(x, (-1, self.num_base_capsules, self.dim_base_capsules)) # u.shape: (None, 1152, 8)\n",
    "            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n",
    "            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n",
    "            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n",
    "            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"DynamicRouting\") as scope:\n",
    "            b = tf.zeros((inputs.shape[0], self.num_base_capsules, self.num_labels, 1)) # b.shape: (None, 1152, 10, 1)\n",
    "            for i in range(self.iterations): # self.iterations = 3\n",
    "                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n",
    "                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n",
    "                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n",
    "                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n",
    "                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n",
    "                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n",
    "                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n",
    "                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n",
    "                # Now matmul is performed in the last two dimensions, and others are broadcasted\n",
    "                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n",
    "                b += agreement\n",
    "        return v\n",
    "\n",
    "    @tf.function\n",
    "    def regenerate_image(self, inputs):\n",
    "        with tf.name_scope(\"Reconstruction\") as scope:\n",
    "            v_ = tf.reshape(inputs, [-1, self.num_labels * self.dim_super_capsules]) # v_.shape: (None, 160)\n",
    "            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n",
    "            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n",
    "            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n",
    "        return reconstructed_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No `profiler_outdir` passed to trace_on(). Profiler won't be enabled.\n"
     ]
    }
   ],
   "source": [
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "model = CapsuleNetwork(\n",
    "    num_filters=num_filters,\n",
    "    num_base_mappings=num_base_mappings,\n",
    "    dim_base_capsules=dim_base_capsules,\n",
    "    num_labels=num_labels,\n",
    "    dim_super_capsules=dim_super_capsules,\n",
    "    iterations=capsule_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimiser = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n"
     ]
    }
   ],
   "source": [
    "def train(X, y) -> float:\n",
    "    y_one_hot = tf.one_hot(y, depth=num_labels)\n",
    "    with tf.GradientTape() as tape:\n",
    "        vector, reconstructed_image = model([X, y_one_hot])\n",
    "        loss = loss_function(vector, reconstructed_image, y_one_hot, X)\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    Optimiser.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# _ = train(X_train[:32], y_train[:32])\n",
    "\n",
    "tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    pred = model.predict_capsule_output(x)\n",
    "    pred_normed = safe_normalise(pred)\n",
    "    pred_normed = tf.squeeze(pred_normed, [1])\n",
    "    return np.argmax(pred_normed, axis=1)[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12: 100%|██████████| 82/82 [00:08<00:00,  6.92it/s, Loss : 0.40833455324172974 Evaluating Accuracy ...]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 920 calls to <function CapsuleNetwork.predict_capsule_output at 0x74f94a7fcfe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12: 100%|██████████| 82/82 [00:10<00:00,  7.61it/s, Loss : 0.40833455324172974 Accuracy : 0.7429064417177914]\n",
      "Epoch 2/12: 100%|██████████| 82/82 [00:10<00:00,  7.67it/s, Loss : 0.4050237238407135 Accuracy : 0.7429064417177914]\n",
      "Epoch 3/12: 100%|██████████| 82/82 [00:10<00:00,  7.60it/s, Loss : 0.4050188660621643 Accuracy : 0.7429064417177914]\n",
      "Epoch 4/12: 100%|██████████| 82/82 [00:09<00:00,  8.76it/s, Loss : 0.40501031279563904 Accuracy : 0.7429064417177914]\n",
      "Epoch 5/12: 100%|██████████| 82/82 [00:10<00:00,  7.63it/s, Loss : 0.40500813722610474 Accuracy : 0.7429064417177914]\n",
      "Epoch 6/12: 100%|██████████| 82/82 [00:10<00:00,  7.85it/s, Loss : 0.40497711300849915 Accuracy : 0.5028757668711656]\n",
      "Epoch 7/12: 100%|██████████| 82/82 [00:10<00:00,  8.06it/s, Loss : 0.40499812364578247 Accuracy : 0.4491947852760736]\n",
      "Epoch 8/12: 100%|██████████| 82/82 [00:09<00:00,  8.60it/s, Loss : 0.40490517020225525 Accuracy : 0.7152990797546013]\n",
      "Epoch 9/12: 100%|██████████| 82/82 [00:10<00:00,  7.63it/s, Loss : 0.4046425223350525 Accuracy : 0.7302530674846626]\n",
      "Epoch 10/12: 100%|██████████| 82/82 [00:09<00:00,  8.58it/s, Loss : 0.40499553084373474 Accuracy : 0.7323619631901841]\n",
      "Epoch 11/12: 100%|██████████| 82/82 [00:09<00:00,  9.01it/s, Loss : 0.4049917459487915 Accuracy : 0.7337039877300614]\n",
      "Epoch 12/12: 100%|██████████| 82/82 [00:09<00:00,  8.45it/s, Loss : 0.4049847722053528 Accuracy : 0.7304447852760736]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "losses = []\n",
    "accuracy = []\n",
    "for i in range(1, epochs+1, 1):\n",
    "\n",
    "    loss = 0\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "\n",
    "        description = f\"Epoch {i}/{epochs}\"\n",
    "        pbar.set_description_str(description)\n",
    "\n",
    "        for X_batch, y_batch in dataset:\n",
    "\n",
    "            loss += train(X_batch, y_batch)\n",
    "            pbar.update(1)\n",
    "\n",
    "        loss /= len(dataset)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        training_sum = 0\n",
    "\n",
    "        print_statement = f\"Loss : {loss.numpy()}\" + \" Evaluating Accuracy ...\"\n",
    "        pbar.set_postfix_str(print_statement)\n",
    "\n",
    "        for X_batch, y_batch in dataset:\n",
    "            training_sum += sum(predict(model, X_batch)==y_batch.numpy())\n",
    "        accuracy.append(training_sum/n_trainset)\n",
    "\n",
    "        print_statement = f\"Loss : {loss.numpy()} Accuracy : {accuracy[-1]}\"\n",
    "\n",
    "        pbar.set_postfix_str(print_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6201923076923077\n"
     ]
    }
   ],
   "source": [
    "test_sum = 0\n",
    "for X_batch, y_batch in testset:\n",
    "    test_sum += sum(predict(model, X_batch)==y_batch.numpy())\n",
    "print(test_sum/n_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/capsnet_tiny_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"capsule_network_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"capsule_network_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ConvolutionLayer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BaseCapsule (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">803,600</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ConvolutionLayer (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m20,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BaseCapsule (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m5,308,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m784\u001b[0m)              │       \u001b[38;5;34m803,600\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,675,472</span> (25.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,675,472\u001b[0m (25.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,675,472</span> (25.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,675,472\u001b[0m (25.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 2 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[511], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m predict(model, tf\u001b[38;5;241m.\u001b[39mexpand_dims(X_test[idx], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     15\u001b[0m temp_features[:,:,:,:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m temp_features[idx:,:,idx,:] \u001b[38;5;241m=\u001b[39m temp_[idx:,:,idx,:]\n\u001b[1;32m     18\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mregenerate_image(temp_features[idx])\n\u001b[1;32m     19\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(reconstruction, (img_size, img_size))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 2 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# n_samples = 5\n",
    "\n",
    "# test_sample = X_test[:n_samples]\n",
    "# features = model.predict_capsule_output(test_sample)\n",
    "\n",
    "# temp_features = features.numpy()\n",
    "# temp_ = temp_features.copy()\n",
    "\n",
    "\n",
    "# label_selection = n_samples\n",
    "# plt.figure(figsize=(label_selection * 2, 3))\n",
    "# for idx in range(label_selection):\n",
    "#     pred = predict(model, tf.expand_dims(X_test[idx], axis=0))\n",
    "\n",
    "#     temp_features[:,:,:,:] = 0\n",
    "#     temp_features[idx:,:,idx,:] = temp_[idx:,:,idx,:]\n",
    "\n",
    "#     reconstruction = model.regenerate_image(temp_features[idx])\n",
    "#     reconstruction = tf.reshape(reconstruction, (img_size, img_size))\n",
    "\n",
    "#     plt.subplot(1, label_selection, idx + 1)\n",
    "#     plt.imshow(reconstruction, cmap=\"gray\")\n",
    "#     plt.title(f\"Predicted: {pred}\")\n",
    "#     plt.axis(\"off\")\n",
    "# plt.show()   \n",
    "\n",
    "\n",
    "# plt.figure(figsize=(label_selection * 2, 3))\n",
    "# for idx in range(label_selection):\n",
    "#     label = y_test[idx]\n",
    "#     plt.subplot(1, label_selection, idx + 1)\n",
    "#     plt.imshow(test_sample[idx], cmap=\"gray\")\n",
    "#     plt.title(f\"Label: {label}\")\n",
    "#     plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# feats = model.predict_capsule_output(\n",
    "#     tf.expand_dims(X_test[idx], axis=0)\n",
    "# )\n",
    "# temp_feats = feats.numpy()\n",
    "\n",
    "# col = np.zeros((img_size, 308))\n",
    "# for i in range(16): \n",
    "#     print(temp_features.shape)\n",
    "#     feature_ = feats.numpy().copy()\n",
    "#     feature_[:,:,idx, i] += -0.25\n",
    "#     row = np.zeros((img_size, img_size))\n",
    "#     for j in range(10):\n",
    "#         feature_[:,:,idx, j] += 0.05\n",
    "#         feature_ = tf.convert_to_tensor(feature_)\n",
    "#         print(feature_.shape)\n",
    "#         print(model.regenerate_image(feature_).shape)\n",
    "#         row = np.hstack([\n",
    "#             row, \n",
    "#             tf.reshape(model.regenerate_image(feature_), \n",
    "#             (img_size, img_size)).numpy()\n",
    "#         ])\n",
    "#     col = np.vstack([col, row])\n",
    "    \n",
    "# plt.figure(figsize=(30,20))\n",
    "# plt.imshow(col[img_size:, img_size:], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
